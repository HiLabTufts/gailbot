'''
	Script that analyses a given audio file for laughter.
	Part of the Gailbot-3 development project.

	Developed by:

		Muhammad Umair								
		Tufts University
		Human Interaction Lab at Tufts

	Initial development: 6/8/19	
'''

import argparse 								# Library to extract input arguments
import os, sys 									# General system libraries.
import librosa									# Audio signal processing library.
import keras 									# Deep learning framework.

import matplotlib.pyplot as plt 				# Library to visualize mfcc features.
import librosa.display 							# Library to display signal.
import numpy 									# Library to have multi-dimensional homogenous arrays.


# *** Global variables / invariants ***
AUDIO_SAMPLE_RATE = 44100


# *** Main driver functions ***

def analyzeLaugh(infoList,threshold,midLength):
	for dic in infoList:
		print(dic['outputDir']+'/'+dic['audioFile'])
		segmentLaugh(dic['outputDir']+'/'+dic['audioFile'],'./LSTM_ThreeLayer_100Epochs.h5',
			dic['outputDir'],0.4,0.05)
		sys.exit()

def segmentLaugh(audioFile, model_path, output_path,threshold, min_length):
	print("Analyzing laughter: {0}".format(audioFile))

	# Loading the audio signal as a time series and obtaining its sampling rate.
	timeSeries, samplingRate = librosa.load(audioFile,sr =AUDIO_SAMPLE_RATE)

	# Loading the existing trained and compiled model to detect laughter.
	#keras.models.load_model(model_path)

	# Getting a list of different audio features for analysis.
	featureList = getFeatureList(timeSeries,samplingRate)


def getFeatureList(timeSeries,samplingRate):
	
	# Computing MFCC features.
	mfccFeatures = computeMfccFeatures(timeSeries,samplingRate)

	# Computing delta features.


# *** Helper functions ***

# Function that extracts mfcc features for the given time series.
'''
	MFCC: Mel frequency cepstral coefficients.
	The MFCC are generated by using a fourier transform to convert the time series
	into the frequency domain and then taking the spectrum of this log using a
	cosine tranformation. 
	The resulting spectrum is in the qufrequency domain.
	It has a peak wherever there is a PERIODIC element in the original time series.
	This mel-scale used in the final transform is a perceptual scale based on 
	what human subjects can hear. It measures the percieved distance.

'''
def computeMfccFeatures(timeSeries, samplingRate):

	# Extractign the mel-frequency coefficients.
	# DCT type-II transform is used and 30 frequency bins are created.
	# Also computing a mel-sclaed spectogram.
	# Hop-length is the number of samples between successive frames. / columns of a spectogram.
	# The .T attribute is the transpose of the numpy array
	mfccFeatures = librosa.feature.mfcc(y=timeSeries,sr=samplingRate,
		n_mfcc=12,n_mels=12,hop_length=int(samplingRate/100),dct_type=2,n_fft=int(samplingRate/40)).T
	print("MFCC features\n{}".format(mfccFeatures))

	# Separating the complex valued Spectrogram D into its magnitude and phase components.
	# A complex valued spectogram does not have any negative frequency components.
	complexValuedMatrix = librosa.stft(timeSeries,hop_length = int(samplingRate/100))
	magnitude,phase = librosa.magphase(complexValuedMatrix)

	# Calculating the root-mean-square value / mean of the cosing function
	# Transposing the resultant matrix.
	rms = librosa.feature.rmse(S=magnitude).T

	# Visualizing the mfcc features.
	plt.figure(figsize=(10, 4))
	librosa.display.specshow(mfccFeatures, x_axis='time') 
	plt.colorbar()
	plt.title('MFCC')
	plt.tight_layout()

	#Visulaizing the original time series.
	fig, ax = plt.subplots()
	plt.title('Time Series')
	ax.plot(timeSeries)

	# Visualizing the complex valued matrix D as a spectogram
	fig, ax = plt.subplots()
	plt.title('RMS signal')
	ax.plot(rms)
	plt.show()


	# stacking the arrays horizontally and returns resultant feature list.
	return numpy.hstack([mfccFeatures,rms])

# Run this script directly to extract laughter from audio.
if __name__ == '__main__':
	dic = {"outputDir" : "pair-0",
			"jsonFile" : "pair-0/test2a-json.txt",
			"audioFile" : "test2a-test2b-combined.wav",
			"names" : ["SP1"]}

	infoList = [dic]
	analyzeLaugh(infoList,0.4,0.05)


























